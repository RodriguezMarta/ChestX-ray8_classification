{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm.notebook import tqdm # Progession bar\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = os.path.abspath('../src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', 'data')\n",
    "images_dir = os.path.join(data_dir,'images')\n",
    "metadata_dir = os.path.join(data_dir, 'metadata')\n",
    "test_csv = os.path.join(metadata_dir, 'test_metadata.csv')\n",
    "train_csv = os.path.join(metadata_dir, 'train_metadata.csv')\n",
    "val_csv = os.path.join(metadata_dir, 'val_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m dataloaders, dataset_sizes, class_counts \u001b[39m=\u001b[39m make_data_loaders(train_csv, \n\u001b[0;32m      3\u001b[0m                                                                 val_csv,\n\u001b[0;32m      4\u001b[0m                                                                 test_csv, \n\u001b[0;32m      5\u001b[0m                                                                 images_dir, \n\u001b[0;32m      6\u001b[0m                                                                 \u001b[39m32\u001b[39;49m, \n\u001b[0;32m      7\u001b[0m                                                                 \u001b[39m224\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\MEDHYCON\\Documents\\Marta\\TFM\\ChestX-ray8_classification\\src\\utils.py:43\u001b[0m, in \u001b[0;36mmake_data_loaders\u001b[1;34m(train_df, val_df, test_df, image_dir, batch_size, image_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m train_transforms \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     30\u001b[0m     transforms\u001b[39m.\u001b[39mResize(image_size),\n\u001b[0;32m     31\u001b[0m     transforms\u001b[39m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     normalize\n\u001b[0;32m     35\u001b[0m ])\n\u001b[0;32m     37\u001b[0m val_transforms \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     38\u001b[0m     transforms\u001b[39m.\u001b[39mResize(image_size),\n\u001b[0;32m     39\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     40\u001b[0m     normalize\n\u001b[0;32m     41\u001b[0m ])\n\u001b[1;32m---> 43\u001b[0m train_dataset \u001b[39m=\u001b[39m ChestXRayDataset(df_dir\u001b[39m=\u001b[39;49mtrain_df, image_dir\u001b[39m=\u001b[39;49mimage_dir, transform\u001b[39m=\u001b[39;49mtrain_transforms)\n\u001b[0;32m     44\u001b[0m val_dataset \u001b[39m=\u001b[39m ChestXRayDataset(df_dir\u001b[39m=\u001b[39mval_df, image_dir\u001b[39m=\u001b[39mimage_dir, transform\u001b[39m=\u001b[39mval_transforms)\n\u001b[0;32m     45\u001b[0m test_dataset \u001b[39m=\u001b[39m ChestXRayDataset(df_dir\u001b[39m=\u001b[39mtest_df, image_dir\u001b[39m=\u001b[39mimage_dir, transform\u001b[39m=\u001b[39mval_transforms)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, \n",
    "                                                                val_csv,\n",
    "                                                                test_csv, \n",
    "                                                                images_dir, \n",
    "                                                                32, \n",
    "                                                                224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.array(class_counts)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "criterion = get_loss('bce_w', counts, device)\n",
    "optimizer = get_optimizer(model.parameters(), optimizer='Adam', lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = get_scheduler(optimizer, name='cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join('..','models','resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "def train_model(device, model, model_dir, train_loader, val_loader, criterion, optimizer,scheduler, num_epochs, steps=None, s_patience=3, patience=15):\n",
    "    model.to(device)\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    start_epoch, best_val_loss = load_checkpoint(model, optimizer, scheduler, model_dir)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        print(f'Starting epoch {epoch}/{start_epoch + num_epochs - 1}')\n",
    "        \n",
    "        start_time = time.time() \n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            if steps and (i >= steps):\n",
    "                break\n",
    "\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.CyclicLR):\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "        train_time = time.time() - start_time  \n",
    "        start_time_val = time.time() \n",
    "\n",
    "        val_loss, val_auc, val_precision, val_recall, val_f1 = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        val_time = time.time() - start_time_val \n",
    "\n",
    "        epoch_time = time.time() - start_time \n",
    "\n",
    "        print(f'Epoch [{epoch}/{num_epochs + start_epoch - 1}], Validation Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, '\n",
    "              f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}, '\n",
    "              f'Training Time: {train_time:.2f}s, Validation Time: {val_time:.2f}s, Total Time: {epoch_time:.2f}s')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f'No improvement in validation loss for {epochs_without_improvement} epoch(s).')\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epochs_without_improvement} epochs without improvement.')\n",
    "            break\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        current_history = pd.DataFrame({'epoch': [epoch],\n",
    "                                        'val_loss': [val_loss],\n",
    "                                        'val_auc': [val_auc],\n",
    "                                        'precision': [val_precision],\n",
    "                                        'recall': [val_recall],\n",
    "                                        'f1_score': [val_f1],\n",
    "                                        'lr': [current_lr],\n",
    "                                        'train_time': [train_time],\n",
    "                                        'val_time': [val_time],\n",
    "                                        'epoch_time': [epoch_time]})\n",
    "        \n",
    "        current_history.to_csv(os.path.join(model_dir, 'history.csv'), mode='a', header=False, index=False)\n",
    "\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, model_dir, best_val_loss)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Training complete. Best Validation Loss:', best_val_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, 'best_model.pth'))\n",
    "    print(f'Best model saved to {os.path.join(model_dir, \"best_model.pth\")}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader) \n",
    "    all_outputs = np.concatenate(all_outputs) \n",
    "    all_labels = np.concatenate(all_labels) \n",
    "    all_preds = (all_outputs > 0.5).astype(int)\n",
    "\n",
    "    auc_scores = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        if np.unique(all_labels[:, i]).size > 1: \n",
    "            auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "            auc_scores.append(auc)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "\n",
    "    mean_auc = np.nanmean(auc_scores)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='micro', zero_division=1)\n",
    "    recall = recall_score(all_labels, all_preds, average='micro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "\n",
    "    return val_loss, mean_auc, precision, recall, f1  \n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, model_dir, best_val_loss):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(model_dir, 'checkpoint.pth'))\n",
    "    print(f'Model checkpoint saved at epoch {epoch}.')\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, model_dir):\n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}.\")\n",
    "        return checkpoint['epoch'] + 1, checkpoint['best_val_loss']\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 1, float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "steps = None\n",
    "s_patience = 3\n",
    "patience = 15\n",
    "\n",
    "model = train(device, \n",
    "                    model, \n",
    "                    model_dir,\n",
    "                    dataloaders['train'], \n",
    "                    dataloaders['val'],\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    num_epochs,\n",
    "                    steps,\n",
    "                    s_patience,\n",
    "                    patience)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
