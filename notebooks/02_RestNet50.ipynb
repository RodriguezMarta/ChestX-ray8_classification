{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "images_dir = '/content/drive/MyDrive/Colab Notebooks/JULE/data/images_224x224'\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/JULE/data/output_multilabel'\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "eVhspSU3iXfW",
        "outputId": "94d63fc3-423d-43ae-df87-1dc141ae26eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib pandas scikit-learn\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "u3UPmUyziwxW",
        "outputId": "68f9dd03-7b22-42f7-8fcc-a8e783e0bb6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "2m92rBQLi1nd",
        "outputId": "5b01dcdf-3017-422a-8cb1-f155e0a4ad66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x).view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "H6ZwkNKGjT4Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DEC(nn.Module):\n",
        "    def __init__(self, feature_extractor, n_labels=14, alpha=1.0):\n",
        "        super(DEC, self).__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.n_labels = n_labels\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Determinación de la dimensión de las características\n",
        "        dummy_input = torch.zeros(1, 3, 224, 224).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = self.feature_extractor(dummy_input)\n",
        "            feature_dim = output.shape[1]\n",
        "\n",
        "        # Inicialización de los centroides\n",
        "        self.centroids = nn.Parameter(torch.randn(n_labels, feature_dim).to(device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)  # Extraer características\n",
        "        distances = torch.cdist(features, self.centroids)  # Calcular distancias a los centroides\n",
        "        probabilities = torch.sigmoid(-distances)  # Convertir a probabilidades\n",
        "        return probabilities\n",
        "\n",
        "    def loss(self, probabilities, targets):\n",
        "        bce_loss = nn.BCELoss()  # Pérdida binaria para cada etiqueta\n",
        "        return bce_loss(probabilities, targets)\n",
        "\n"
      ],
      "metadata": {
        "id": "mS8EjDa3jW_R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_dec_multilabel(dataloader, model, optimizer, n_epochs=20, output_dir=\"output\"):\n",
        "    metrics = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        total_hamming = 0\n",
        "        total_f1_macro = 0\n",
        "        total_f1_micro = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            probabilities = model(inputs)\n",
        "            loss = model.loss(probabilities, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predictions = (probabilities > 0.5).float()  # Umbral para predicciones\n",
        "            hamming = hamming_loss(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            f1_macro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"macro\")\n",
        "            f1_micro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"micro\")\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_hamming += hamming * len(inputs)\n",
        "            total_f1_macro += f1_macro * len(inputs)\n",
        "            total_f1_micro += f1_micro * len(inputs)\n",
        "            total_samples += len(inputs)\n",
        "\n",
        "        # Calcular métricas promedio\n",
        "        epoch_loss = total_loss / len(dataloader)\n",
        "        epoch_hamming = total_hamming / total_samples\n",
        "        epoch_f1_macro = total_f1_macro / total_samples\n",
        "        epoch_f1_micro = total_f1_micro / total_samples\n",
        "\n",
        "        metrics.append({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Loss\": epoch_loss,\n",
        "            \"Hamming Loss\": epoch_hamming,\n",
        "            \"F1 Macro\": epoch_f1_macro,\n",
        "            \"F1 Micro\": epoch_f1_micro\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Hamming Loss: {epoch_hamming:.4f}, F1 Macro: {epoch_f1_macro:.4f}, \"\n",
        "              f\"F1 Micro: {epoch_f1_micro:.4f}\")\n",
        "\n",
        "    # Guardar métricas y modelo\n",
        "    metrics_file = os.path.join(output_dir, \"metrics.csv\")\n",
        "    pd.DataFrame(metrics).to_csv(metrics_file, index=False)\n",
        "\n",
        "    model_file = os.path.join(output_dir, \"dec_model_multilabel.pth\")\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "\n",
        "    print(f\"Entrenamiento completado en {time.time() - start_time:.2f} segundos. \"\n",
        "          f\"Modelo y métricas guardados en {output_dir}.\")\n"
      ],
      "metadata": {
        "id": "PFEasHlBjtOO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DGxZuaLCh_0E"
      },
      "outputs": [],
      "source": [
        "def evaluate_dec_multilabel(model, dataloader):\n",
        "    model.eval()\n",
        "    total_hamming = 0\n",
        "    total_f1_macro = 0\n",
        "    total_f1_micro = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            probabilities = model(inputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "\n",
        "            hamming = hamming_loss(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            f1_macro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"macro\")\n",
        "            f1_micro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"micro\")\n",
        "\n",
        "            total_hamming += hamming * len(inputs)\n",
        "            total_f1_macro += f1_macro * len(inputs)\n",
        "            total_f1_micro += f1_micro * len(inputs)\n",
        "            total_samples += len(inputs)\n",
        "\n",
        "    return {\n",
        "        \"Hamming Loss\": total_hamming / total_samples,\n",
        "        \"F1 Macro\": total_f1_macro / total_samples,\n",
        "        \"F1 Micro\": total_f1_micro / total_samples\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 7: Visualizar las Métricas\n",
        "def plot_metrics(metrics_file):\n",
        "    metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics_df[\"Epoch\"], metrics_df[\"Loss\"], label=\"Loss\")\n",
        "    plt.plot(metrics_df[\"Epoch\"], metrics_df[\"Hamming Loss\"], label=\"Hamming Loss\")\n",
        "    plt.plot(metrics_df[\"Epoch\"], metrics_df[\"F1 Macro\"], label=\"F1 Macro\")\n",
        "    plt.plot(metrics_df[\"Epoch\"], metrics_df[\"F1 Micro\"], label=\"F1 Micro\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training Metrics\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "tz3PDtOij8gy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "# Transformaciones para las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std =[0.5])\n",
        "])\n",
        "dataset = ImageFolder(root=images_dir, transform=transform)\n",
        "\n",
        "# Dividir el conjunto de datos en subconjuntos aleatorios\n",
        "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
        "test_size = len(dataset) - train_size  # 20% para prueba\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Crear DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {len(train_dataset)}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "d0NkzzWBkS5T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=train_list_path,\n",
        "    mode='train',  # Training mode\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=test_list_path,\n",
        "    mode='test',  # Training mode\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "CQ5d7OAkkGMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=train_list_path,\n",
        "    mode='train',  # Training mode\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=test_list_path,\n",
        "    mode='test',  # Training mode\n",
        "    transform=transform\n",
        ")\n",
        "train_loader =  DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "SIz6oGgvkDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_extractor = FeatureExtractor(pretrained=True).to(device)\n",
        "dec_model = DEC(feature_extractor, n_labels=14).to(device)\n",
        "optimizer = torch.optim.Adam(dec_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Entrenamiento\n",
        "train_dec_multilabel(train_loader, dec_model, optimizer, n_epochs=20, output_dir=output_dir)\n",
        "\n",
        "# Evaluación\n",
        "metrics = evaluate_dec_multilabel(dec_model, test_loader)\n",
        "print(\"Metrics on Test Set:\", metrics)\n",
        "\n",
        "# Graficar métricas\n",
        "plot_metrics(os.path.join(output_dir, \"metrics.csv\"))"
      ],
      "metadata": {
        "id": "KFrkuCXAkBWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaLbulCEh_0F"
      },
      "outputs": [],
      "source": [
        "train_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=train_list_path,\n",
        "    mode='train',  # Training mode\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = ChestXray8Dataset(\n",
        "    img_dir=images_dir,\n",
        "    metadata_file=metadata_dir,\n",
        "    split_file=test_list_path,\n",
        "    mode='test',  # Training mode\n",
        "    transform=transform\n",
        ")\n",
        "train_loader =  DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6cC_oMsh_0F",
        "outputId": "1f874c7c-6f4d-4e7a-a72a-8db2ea8663ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Forward\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     23\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    455\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_classes = 14  # Número de etiquetas en tu dataset\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, num_classes),\n",
        "    nn.Sigmoid()  # Usamos sigmoid porque es una tarea multietiqueta\n",
        ")\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}