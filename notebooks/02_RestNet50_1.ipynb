{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../src') \n",
    "from data_loader import ChestXray8Dataset\n",
    "from models.cnn import ResNet50\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm # Progession bar\n",
    "\n",
    "# Configuration settings\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "images_dir = data_dir / 'images'\n",
    "metadata_dir = data_dir /'metadata'/ 'Data_Entry_2017_v2020.csv'\n",
    "train_list_path = data_dir /'metadata'/ 'train_val_list.txt'\n",
    "test_list_path = data_dir / 'metadata' /'test_list.txt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(metadata_dir)\n",
    "# Obtener las clases de las etiquetas\n",
    "classes = [\"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\",\n",
    "           \"Edema\", \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\", \n",
    "           \"Pleural_Thickening\", \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\"]\n",
    "\n",
    "# Crear una columna binaria por clase\n",
    "for cls in classes:\n",
    "    metadata_df[cls] = metadata_df['Finding Labels'].apply(lambda x: 1 if cls in x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_val_dataset = ChestXray8Dataset(\n",
    "    img_dir=images_dir, \n",
    "    metadata_file=metadata_dir, \n",
    "    split_file=train_list_path,\n",
    "    mode='train',  # Training mode\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),             # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),                     # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalización\n",
    "])\n",
    "\n",
    "test_dataset = ChestXray8Dataset(\n",
    "    img_dir=images_dir, \n",
    "    metadata_file=metadata_dir, \n",
    "    split_file=test_list_path,\n",
    "    mode='test',  # Training mode\n",
    "    transform=transform\n",
    ")\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Calcular los pesos inversos de las etiquetas\n",
    "class_counts = metadata_df[classes].sum(axis=0).values\n",
    "weights = 1.0 / class_counts\n",
    "samples_weights = [weights[row[classes].values.argmax()] for _, row in metadata_df.iterrows()]\n",
    "\n",
    "# Crear sampler\n",
    "sampler = WeightedRandomSampler(samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# Usar sampler en DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Congelar todas las capas excepto la última\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modificar la última capa para 14 clases\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(classes))  # 14 clases\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Focal Loss\n",
    "# -----------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "        )\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        probs = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "        loss = self.alpha * focal_weight * bce_loss\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# Pesos de las clases\n",
    "pos_weights = torch.tensor(class_counts.max() / class_counts, dtype=torch.float32).to(device)\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2, pos_weight=pos_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import csv\n",
    "\n",
    "# Función para calcular métricas por batch (AUC y F1-Score)\n",
    "def compute_metrics(outputs, labels):\n",
    "    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    # AUC y F1 por clase\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    for i in range(outputs.shape[1]):  # Loop por clases\n",
    "        try:\n",
    "            auc = roc_auc_score(labels[:, i], outputs[:, i])\n",
    "            f1 = f1_score(labels[:, i], outputs[:, i] > 0.5)\n",
    "        except ValueError:\n",
    "            auc, f1 = 0.0, 0.0  # En caso de que una clase no esté representada en el batch\n",
    "        aucs.append(auc)\n",
    "        f1s.append(f1)\n",
    "    return np.mean(aucs), np.mean(f1s)\n",
    "\n",
    "# Guardar las métricas en un CSV\n",
    "def save_metrics(epoch, train_loss, val_loss, train_auc, val_auc, train_f1, val_f1, file_path):\n",
    "    with open(file_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if epoch == 0:\n",
    "            writer.writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Train_AUC', 'Val_AUC', 'Train_F1', 'Val_F1'])\n",
    "        writer.writerow([epoch + 1, train_loss, val_loss, train_auc, val_auc, train_f1, val_f1])\n",
    "\n",
    "# Modificar las funciones de entrenamiento y validación\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        all_outputs.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Calcular métricas\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    auc, f1 = compute_metrics(all_outputs, all_labels)\n",
    "    return avg_loss, auc, f1\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Calcular métricas\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    avg_loss = running_loss / len(val_loader.dataset)\n",
    "    auc, f1 = compute_metrics(all_outputs, all_labels)\n",
    "    return avg_loss, auc, f1\n",
    "\n",
    "# -----------------------------\n",
    "# Entrenamiento del Modelo\n",
    "# -----------------------------\n",
    "num_epochs = 10\n",
    "metrics_file = 'training_metrics_2.csv'\n",
    "\n",
    "# Borrar contenido previo del archivo de métricas\n",
    "with open(metrics_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenamiento\n",
    "    train_loss, train_auc, train_f1 = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    # Validación\n",
    "    val_loss, val_auc, val_f1 = validate_model(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f} | \"\n",
    "          f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Guardar métricas\n",
    "    save_metrics(epoch, train_loss, val_loss, train_auc, val_auc, train_f1, val_f1, metrics_file)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluación del Test Set\n",
    "# -----------------------------\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Calcular métricas\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    avg_loss = running_loss / len(test_loader.dataset)\n",
    "    auc, f1 = compute_metrics(all_outputs, all_labels)\n",
    "    return avg_loss, auc, f1\n",
    "\n",
    "# Test loader (igual que val_loader)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_loss, test_auc, test_f1 = test_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test AUC: {test_auc:.4f} | Test F1: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
