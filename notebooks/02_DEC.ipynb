{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../src') \n",
    "from data_loader import ChestXray8Dataset\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm # Progession bar\n",
    "\n",
    "# Configuration settings\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "images_dir = data_dir / 'images'\n",
    "metadata_dir = data_dir /'metadata'/ 'Data_Entry_2017_v2020.csv'\n",
    "train_list_path = data_dir /'metadata'/ 'train_val_list.txt'\n",
    "test_list_path = data_dir / 'metadata' /'test_list.txt'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),             # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),                     # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalización\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXray8Dataset(\n",
    "    img_dir=images_dir, \n",
    "    metadata_file=metadata_dir, \n",
    "    split_file=train_list_path,\n",
    "    mode='train',  # Training mode\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = ChestXray8Dataset(\n",
    "    img_dir=images_dir, \n",
    "    metadata_file=metadata_dir, \n",
    "    split_file=test_list_path,\n",
    "    mode='test',  # Training mode\n",
    "    transform=transform\n",
    ")\n",
    "train_loader =  DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Quitar capa final\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x).view(x.size(0), -1)  # Aplanar las características\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DEC(nn.Module):\n",
    "    def __init__(self, feature_extractor, n_labels=14, alpha=1.0):\n",
    "        super(DEC, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.n_labels = n_labels\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Cálculo dinámico de la dimensión de salida\n",
    "        dummy_input = torch.zeros(1, 3, 224, 224).cuda()  # Mover dummy_input a GPU\n",
    "        with torch.no_grad():\n",
    "            output = self.feature_extractor(dummy_input)\n",
    "            feature_dim = output.shape[1]\n",
    "\n",
    "        self.centroids = nn.Parameter(torch.randn(n_labels, feature_dim).cuda())  # Centroides en GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        distances = torch.cdist(features, self.centroids)\n",
    "        probabilities = torch.sigmoid(-distances)\n",
    "        return probabilities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def train_dec_multilabel(dataloader, model, optimizer, n_epochs=20, output_dir=\"output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metrics_file = os.path.join(output_dir, \"metrics.csv\")\n",
    "    model_file = os.path.join(output_dir, \"dec_model_multilabel.pth\")\n",
    "\n",
    "    metrics = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_hamming = 0\n",
    "        total_f1_macro = 0\n",
    "        total_f1_micro = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            probabilities = model(inputs)\n",
    "            loss = model.loss(probabilities, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            hamming = hamming_loss(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            f1_macro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"macro\")\n",
    "            f1_micro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"micro\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_hamming += hamming * len(inputs)\n",
    "            total_f1_macro += f1_macro * len(inputs)\n",
    "            total_f1_micro += f1_micro * len(inputs)\n",
    "            total_samples += len(inputs)\n",
    "\n",
    "        epoch_loss = total_loss / len(dataloader)\n",
    "        epoch_hamming = total_hamming / total_samples\n",
    "        epoch_f1_macro = total_f1_macro / total_samples\n",
    "        epoch_f1_micro = total_f1_micro / total_samples\n",
    "\n",
    "        metrics.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Loss\": epoch_loss,\n",
    "            \"Hamming Loss\": epoch_hamming,\n",
    "            \"F1 Macro\": epoch_f1_macro,\n",
    "            \"F1 Micro\": epoch_f1_micro\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Hamming Loss: {epoch_hamming:.4f}, F1 Macro: {epoch_f1_macro:.4f}, \"\n",
    "              f\"F1 Micro: {epoch_f1_micro:.4f}\")\n",
    "\n",
    "    pd.DataFrame(metrics).to_csv(metrics_file, index=False)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    print(f\"Entrenamiento completado en {time.time() - start_time:.2f} segundos. \"\n",
    "          f\"Modelo y métricas guardados en {output_dir}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dec_multilabel(model, dataloader):\n",
    "    model.eval()\n",
    "    total_hamming = 0\n",
    "    total_f1_macro = 0\n",
    "    total_f1_micro = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            probabilities = model(inputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "\n",
    "            hamming = hamming_loss(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            f1_macro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"macro\")\n",
    "            f1_micro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average=\"micro\")\n",
    "\n",
    "            total_hamming += hamming * len(inputs)\n",
    "            total_f1_macro += f1_macro * len(inputs)\n",
    "            total_f1_micro += f1_micro * len(inputs)\n",
    "            total_samples += len(inputs)\n",
    "\n",
    "    return {\n",
    "        \"Hamming Loss\": total_hamming / total_samples,\n",
    "        \"F1 Macro\": total_f1_macro / total_samples,\n",
    "        \"F1 Micro\": total_f1_micro / total_samples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "X1 and X2 must have the same number of columns. X1: 1 X2: 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(dec_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Entrenamiento\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_dec_multilabel(train_loader, dec_model, optimizer, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput_multilabel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Evaluación\u001b[39;00m\n\u001b[0;32m     13\u001b[0m metrics \u001b[39m=\u001b[39m evaluate_dec_multilabel(dec_model, test_loader)\n",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m, in \u001b[0;36mtrain_dec_multilabel\u001b[1;34m(dataloader, model, optimizer, n_epochs, output_dir)\u001b[0m\n\u001b[0;32m     23\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m probabilities \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     28\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(probabilities, labels)\n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mDEC.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor(x)\n\u001b[1;32m---> 29\u001b[0m     distances \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcdist(features, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcentroids)\n\u001b[0;32m     30\u001b[0m     probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39m-\u001b[39mdistances)\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m probabilities\n",
      "File \u001b[1;32mc:\\Users\\MEDHYCON\\anaconda3\\envs\\tfm_env\\lib\\site-packages\\torch\\functional.py:1336\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1334\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[39m=\u001b[39mp, compute_mode\u001b[39m=\u001b[39mcompute_mode)\n\u001b[0;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m compute_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m-> 1336\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mcdist(x1, x2, p, \u001b[39mNone\u001b[39;49;00m)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39melif\u001b[39;00m compute_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39muse_mm_for_euclid_dist\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1338\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mcdist(x1, x2, p, \u001b[39m1\u001b[39m)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: X1 and X2 must have the same number of columns. X1: 1 X2: 2048"
     ]
    }
   ],
   "source": [
    "\n",
    "# Modelo\n",
    "feature_extractor = models.resnet50(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])  # Quitar capa final\n",
    "feature_extractor = feature_extractor.cuda()\n",
    "\n",
    "dec_model = DEC(feature_extractor, n_labels=14).cuda()\n",
    "optimizer = torch.optim.Adam(dec_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Entrenamiento\n",
    "train_dec_multilabel(train_loader, dec_model, optimizer, n_epochs=20, output_dir=\"output_multilabel\")\n",
    "\n",
    "# Evaluación\n",
    "metrics = evaluate_dec_multilabel(dec_model, test_loader)\n",
    "print(\"Metrics on Test Set:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
